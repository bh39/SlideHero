{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ralVGKf_NXCm"
      },
      "source": [
        "## ThirdAI's NeuralDB\n",
        "\n",
        "First let's import the relevant module and initialize a neural db class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ThirdAILabs/Demos.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpJiu7CmSR62",
        "outputId": "ac57bb67-ba94-4665-a845-17d70c2de3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Demos' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thirdai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z42YnT9MNxTb",
        "outputId": "f9e6ae04-e19e-4d0b-cdfa-6f5ee1360a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting thirdai\n",
            "  Downloading thirdai-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thirdai) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from thirdai) (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from thirdai) (2.27.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from thirdai) (1.5.3)\n",
            "Collecting PyTrie (from thirdai)\n",
            "  Downloading PyTrie-0.4.0.tar.gz (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai) (2022.7.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from PyTrie->thirdai) (2.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->thirdai) (1.16.0)\n",
            "Building wheels for collected packages: PyTrie\n",
            "  Building wheel for PyTrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyTrie: filename=PyTrie-0.4.0-py3-none-any.whl size=6081 sha256=3020e5a221c3485919a1451a49a3bc442ab699cd7155dbf5415e6978a2083438\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/0e/a3/3563272cb57af4afbc50c4c7882dd4540944aadde25c82bd45\n",
            "Successfully built PyTrie\n",
            "Installing collected packages: PyTrie, thirdai\n",
            "Successfully installed PyTrie-0.4.0 thirdai-0.7.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpjjx41gNXCo"
      },
      "outputs": [],
      "source": [
        "from thirdai import licensing\n",
        "licensing.activate(\"15EABF-7989A6-794152-D11EEF-8DEB68-V3\")\n",
        "\n",
        "from thirdai import neural_db as ndb\n",
        "\n",
        "db = ndb.NeuralDB(user_id=\"pengcheng0721\") # you can use any username, in the future, this username will let you push models to the model hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55FJfsi9NXCp"
      },
      "source": [
        "### Initialize\n",
        "\n",
        "At this point, the db is uninitialized.\n",
        "\n",
        "##### Option 1: We can either initialize from scratch like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atdYWFN2NXCp"
      },
      "outputs": [],
      "source": [
        "db.from_scratch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PajGkKQ-NXCq"
      },
      "source": [
        "##### Option 2: Or even load from a base DB that we provide, as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNmqX50iNXCq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "checkpoint = \"qna_db_1\"\n",
        "\n",
        "if not os.path.exists(checkpoint):\n",
        "    os.system(\"wget -O qna_db_1.zip 'https://www.dropbox.com/scl/fi/s1zhxmwjpayj5jphzct0p/qna_1_db.zip?dl=0&rlkey=ftcgrzt1rpc2d6hx0iuk1lz1r'\")\n",
        "    os.system(\"unzip qna_db_1.zip -d qna_db_1\")\n",
        "\n",
        "db.from_checkpoint(\"qna_db_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aash5cL8NXCq"
      },
      "source": [
        "### Prep CSV data\n",
        "\n",
        "Let's insert things into it!\n",
        "\n",
        "Currently, we support adding as many CSV files as you wish. All other file formats have to be converted into CSV files where each row represents a paragraph/text-chunk of the document.\n",
        "\n",
        "The file is required to have a column named \"DOC_ID\" with rows numbered from 0 to n_rows-1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Demos/neural_db/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Q801UkTNoC",
        "outputId": "50a3e88d-79ba-4be0-bc14-6f745f46a50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Demos/neural_db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtD_xfy6NXCq"
      },
      "outputs": [],
      "source": [
        "from utils import CSV\n",
        "\n",
        "csv_files = ['/content/pptx_data.csv']\n",
        "csv_docs = []\n",
        "\n",
        "for file in csv_files:\n",
        "    csv_doc = CSV(\n",
        "        path=file,\n",
        "        id_column=\"DOC_ID\",\n",
        "        strong_columns=[\"text_clean\"],\n",
        "        weak_columns=[\"text\"],\n",
        "        reference_columns=[\"text_clean\"])\n",
        "\n",
        "    csv_docs.append(csv_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjagQ1GbNXCq"
      },
      "source": [
        "### Insert CSV files into NeuralDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEDjyPmcNXCr"
      },
      "outputs": [],
      "source": [
        "source_ids = db.insert(csv_docs, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flAFUlJ0NXCr"
      },
      "source": [
        "### Insert and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzrWPuVANXCr",
        "outputId": "37a52abc-57bc-43c7-ab23-da9cb01d500d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2463 | train_hash_precision@5=0.204356  | train_batches 3 | time 359s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2466 | train_hash_precision@5=0.663217  | train_batches 3 | time 348s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2469 | train_hash_precision@5=0.854095  | train_batches 3 | time 346s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2472 | train_hash_precision@5=0.875019  | train_batches 3 | time 343s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2475 | train_hash_precision@5=0.900819  | train_batches 3 | time 342s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2478 | train_hash_precision@5=0.915078  | train_batches 3 | time 347s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2481 | train_hash_precision@5=0.923492  | train_batches 3 | time 346s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2484 | train_hash_precision@5=0.930045  | train_batches 3 | time 343s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2487 | train_hash_precision@5=0.933135  | train_batches 3 | time 342s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2490 | train_hash_precision@5=0.93615  | train_batches 3 | time 347s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2493 | train_hash_precision@5=0.940432  | train_batches 3 | time 345s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2496 | train_hash_precision@5=0.941214  | train_batches 3 | time 341s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2499 | train_hash_precision@5=0.943075  | train_batches 3 | time 345s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2502 | train_hash_precision@5=0.944043  | train_batches 3 | time 345s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2505 | train_hash_precision@5=0.948474  | train_batches 3 | time 344s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2508 | train_hash_precision@5=0.947245  | train_batches 3 | time 339s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2511 | train_hash_precision@5=0.949814  | train_batches 3 | time 339s\n",
            "\n",
            "loaded data | source 'Documents:\n",
            "pptx_data.csv' | vectors 5372 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2514 | train_hash_precision@5=0.951564  | train_batches 3 | time 335s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "source_ids = db.insert(csv_docs, train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opm8HXvKNXCs"
      },
      "source": [
        "### Just train on the docs\n",
        "\n",
        "Do not worry abt files being inserted multiple times, the DB takes care of de-duplication!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF95h4pYNXCs"
      },
      "outputs": [],
      "source": [
        "source_ids = db.insert(csv_docs, train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey6FQvrsNXCs"
      },
      "source": [
        "### Search\n",
        "\n",
        "Now let's start searching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bseBMLwNXCs",
        "outputId": "573c41c5-54ce-4eaa-aeb6-5fa2f2fb064f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cox Concurrency Three Basic Mechanisms for Creating Concurrent Flows 1. Processes Kernel automatically interleaves multiple logical flows Each flow has its own private address space 2. Threads Kernel or thread library automatically interleaves multiple logical flows Each flow shares the same address space 3. IO multiplexing User manually interleaves multiple logical flows Each flow shares the same address space Popular idea for highperformance server designs\n",
            "************\n",
            "Address Translation With a Page Table Virtual Memory Cox\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"What are the three basic mechanisms for creating concurrent flows?\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text())\n",
        "    # print(result.context(radius=3))\n",
        "    # print(result.source())\n",
        "    # print(result.metadata())\n",
        "    # result.show()\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkP6ALNENXCs"
      },
      "source": [
        "We can see that the search pulled up the right passage that contains the termination period \"(i) five (5) years or (ii) when the confidential information no longer qualifies as a trade secret\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al9WgfsrNXCt",
        "outputId": "35c9df54-405f-4867-949e-1b08a648cb9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confidentiality agreement this confidentiality agreement (the “agreement”) is made by and between acme. dba tothemoon inc. with offices at 2025 guadalupe st. suite 260 austin tx 78705 and starwars dba tothemars with offices at the forest moon of endor and entered as of may 3 2023 (“effective date”).\n",
            "************\n",
            "in consideration of the business discussions disclosure of confidential information and any future business relationship between the parties it is hereby agreed as follows: 1. confidential information. for purposes of this agreement the term “confidential information” shall mean any information business plan concept idea know-how process technique program design formula algorithm or work-in-process request for proposal (rfp) or request for information (rfi) and any responses thereto engineering manufacturing marketing technical financial data or sales information or information regarding suppliers customers employees investors or business operations and other information or materials whether disclosed in written graphic oral or electronic form whether tangible or intangible and in whatever form or medium provided or which is learned or disclosed in the course of discussions studies or other work undertaken between the parties prior to or after the effective date.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"made by and between\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text())\n",
        "    # print(result.context(radius=3))\n",
        "    # print(result.source())\n",
        "    # print(result.metadata())\n",
        "    # result.show()\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oReWJDFGNXCt"
      },
      "source": [
        "We can see that the search pulled up the right passage again that has \"made by and between\".\n",
        "\n",
        "Now let's ask a tricky question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bLEYf9TNXCt",
        "outputId": "b1ed6039-d81d-4ae2-9cee-51680a7ee207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3. joint undertaking. each party agrees that it will not at any time disclose give or transmit in any manner or for any purpose the confidential information received from the other party to any person firm or corporation or use such confidential information for its own benefit or the benefit of anyone else or for any purpose other than to engage in discussions regarding a possible business relationship or the current business relationship involving both parties.\n",
            "************\n",
            "6. excluded information. the parties agree that confidential information of the other party shall not include any information to the extent that the information: (i) is or at any time becomes a part of the public domain through no act or omission of the receiving party; (ii) is independently discovered or developed by the receiving party without use of the disclosing party’s confidential information; (iii) is rightfully obtained from a third party without any obligation of confidentiality; or (iv) is already known by the receiving party without any obligation of confidentiality prior to obtaining the confidential information from the disclosing party.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"who are the parties involved?\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text())\n",
        "    # print(result.context(radius=3))\n",
        "    # print(result.source())\n",
        "    # print(result.metadata())\n",
        "    # result.show()\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGPCt2jNXCt"
      },
      "source": [
        "Oops! looks like when we search for \"parties involved\", we do not get the correct paragraph in the 1st position (we should be expecting the first paragraph as the correct results instead fo the last).\n",
        "\n",
        "No worries, we'll show shot to teach the model to correct it's retrieval.\n",
        "\n",
        "### RLHF\n",
        "\n",
        "Let's go over some of NeuralDB's advanced features. The first one is text-to-text association. This allows you to teach the model that two keywords, phrases, or concepts are related.\n",
        "\n",
        "Based on the above example, let's teach the model that \"parties involved\" and the phrase \"made by between\" are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30C9chMLNXCt"
      },
      "outputs": [],
      "source": [
        "db.associate(source=\"parties involved\", target=\"made by and between\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWXr9BEPNXCu"
      },
      "source": [
        "Let's search again with the same query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_J1ApW8NXCu",
        "outputId": "01ea1335-9eb0-424e-caad-2e184c03154b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confidentiality agreement this confidentiality agreement (the “agreement”) is made by and between acme. dba tothemoon inc. with offices at 2025 guadalupe st. suite 260 austin tx 78705 and starwars dba tothemars with offices at the forest moon of endor and entered as of may 3 2023 (“effective date”).\n",
            "************\n",
            "6. excluded information. the parties agree that confidential information of the other party shall not include any information to the extent that the information: (i) is or at any time becomes a part of the public domain through no act or omission of the receiving party; (ii) is independently discovered or developed by the receiving party without use of the disclosing party’s confidential information; (iii) is rightfully obtained from a third party without any obligation of confidentiality; or (iv) is already known by the receiving party without any obligation of confidentiality prior to obtaining the confidential information from the disclosing party.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"who are the parties involved?\",\n",
        "    top_k=2,\n",
        ")\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text())\n",
        "    # print(result.source())\n",
        "    # print(result.metadata())\n",
        "    # result.show()\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rkl308ONXCu"
      },
      "source": [
        "There you go! In just a line, you taught the model to correct itself and retrieve the correct result.\n",
        "\n",
        "Now, let's see the 2nd option which is text-to-result association. Let's say that you know that \"parties involved\" should go the paragraph with DOC_ID=0, you can simply teach the model to associate the query to the corresponding label using the following API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDT3SyJXNXCu"
      },
      "outputs": [],
      "source": [
        "db.text_to_result(\"made by and between\",0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFaY-G0tNXCu"
      },
      "source": [
        "If you want to use the above RLHF methods in a batch instead of a single sample, you can simply use the batched versions of the APIs as shown next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-G4dW80NXCu"
      },
      "outputs": [],
      "source": [
        "db.associate_batch([(\"parties involved\",\"made by and between\"),(\"date of signing\",\"duly executed\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxF1cIndNXCu"
      },
      "outputs": [],
      "source": [
        "db.text_to_result_batch([(\"parties involved\",0),(\"date of signing\",16)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1LpY13QNXCu"
      },
      "source": [
        "### Supervised Training (Optional)\n",
        "\n",
        "If you have supervised data for a specific CSV file in your list, you can simply train the DB on that file by specifying a source_id = source_ids[*file_number_in_your_list*].\n",
        "\n",
        "Note: The supervised file should have the query_column and id_column that you specify in the following call. The id_column should match the id_column that you specified in the \"Prep CSV Data\" step or default to \"DOC_ID\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtXCqs-bNXCv"
      },
      "outputs": [],
      "source": [
        "sup_files = ['sample_nda_sup.csv']\n",
        "\n",
        "db.supervised_train([ndb.Sup(path, query_column=\"QUERY\", id_column=\"DOC_ID\", source_id=source_ids[0]) for path in sup_files])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eug91aMNXCv"
      },
      "source": [
        "### Get Answers from OpenAI using Langchain\n",
        "\n",
        "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above DB. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other generative model of your choice. You can choose to use an source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sswZ7fYvHCwv",
        "outputId": "dda0f9db-7e94-421e-df05-9977deb80530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.213-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.8 frozenlist-1.3.3 langchain-0.0.213 langchainplus-sdk-0.0.17 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paper-qa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmzgxjsbHZfp",
        "outputId": "0a4cb4b5-b48e-4c6b-d812-c3dadcb7c396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting paper-qa\n",
            "  Downloading paper_qa-3.1.0-py3-none-any.whl (28 kB)\n",
            "Collecting pypdf (from paper-qa)\n",
            "  Downloading pypdf-3.11.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.3/256.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain>=0.0.198 in /usr/local/lib/python3.10/dist-packages (from paper-qa) (0.0.213)\n",
            "Collecting openai>=0.27.8 (from paper-qa)\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu (from paper-qa)\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyCryptodome (from paper-qa)\n",
            "  Downloading pycryptodome-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2text (from paper-qa)\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting tiktoken>=0.4.0 (from paper-qa)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.0.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (0.5.8)\n",
            "Requirement already satisfied: langchainplus-sdk>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (0.0.17)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.8->paper-qa) (4.65.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->paper-qa) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain>=0.0.198->paper-qa) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.198->paper-qa) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (1.0.0)\n",
            "Installing collected packages: faiss-cpu, pypdf, PyCryptodome, html2text, tiktoken, openai, paper-qa\n",
            "Successfully installed PyCryptodome-3.18.0 faiss-cpu-1.7.4 html2text-2020.1.16 openai-0.27.8 paper-qa-3.1.0 pypdf-3.11.0 tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "6b4FYDdYIrsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsIY4tJmNXCv"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from paperqa.prompts import qa_prompt\n",
        "from paperqa.chains import make_chain\n",
        "\n",
        "your_openai_key = \"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\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.1,\n",
        "    openai_api_key=your_openai_key,\n",
        ")\n",
        "\n",
        "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4ppfYdwNXCv"
      },
      "outputs": [],
      "source": [
        "def get_references(query):\n",
        "    search_results = db.search(query,top_k=3)\n",
        "    references = []\n",
        "    for result in search_results:\n",
        "        references.append(result.text())\n",
        "    return references\n",
        "\n",
        "def get_answer(query, references):\n",
        "    return qa_chain.run(question=query, context='\\n\\n'.join(references[:3]), answer_length=\"abt 50 words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRn_p4YeNXCv",
        "outputId": "ebfc0713-638b-45d0-ff7c-ada742d2ccdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cox Concurrency Three Basic Mechanisms for Creating Concurrent Flows 1. Processes Kernel automatically interleaves multiple logical flows Each flow has its own private address space 2. Threads Kernel or thread library automatically interleaves multiple logical flows Each flow shares the same address space 3. IO multiplexing User manually interleaves multiple logical flows Each flow shares the same address space Popular idea for highperformance server designs', 'Address Translation With a Page Table Virtual Memory Cox', 'ProducerConsumer on a Buffer That Holds One Item  buf1.c  producerconsumer on 1element buffer  include csapp.h define NITERS 5 void producervoid arg void consumervoid arg struct    int buf  shared var    sem_t full  sems    sem_t empty  shared int main    pthread_t tid_producer   pthread_t tid_consumer     initialize the semaphores    Sem_initshared.empty 0 1    Sem_initshared.full  0 0     create threads and wait    Pthread_createtid_producer NULL                   producer NULL   Pthread_createtid_consumer NULL                   consumer NULL   Pthread_jointid_producer NULL   Pthread_jointid_consumer NULL      exit0  Cox Concurrency']\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the three basic mechanisms for creating concurrent flows?\"\n",
        "\n",
        "references = get_references(query)\n",
        "print(references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjAW6j_JNXCv",
        "outputId": "fb297ee9-b797-4312-ad69-2e8635a271db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three basic mechanisms for creating concurrent flows are processes, threads, and IO multiplexing. Processes are automatically interleaved by the kernel and have their own private address space, while threads are interleaved by either the kernel or thread library and share the same address space. IO multiplexing involves manually interleaving multiple logical flows and is a popular idea for high-performance server designs (Cox Concurrency).\n"
          ]
        }
      ],
      "source": [
        "answer = get_answer(query, references)\n",
        "\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X63ZAtnNXCv"
      },
      "source": [
        "### Load and Save\n",
        "As usual, saving and loading the DB are one-liners."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sTp790JhNXCw",
        "outputId": "40c22fde-986d-4f55-eeb6-12853ddc3e4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sample_nda.db'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# save your db\n",
        "db.save(\"sample_nda.db\")\n",
        "\n",
        "# Loading is just like we showed above, with an optional progress handler\n",
        "#db.from_checkpoint(\"sample_nda.db\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1l8e5o4NXCw"
      },
      "source": [
        "### Export to Playground\n",
        "\n",
        "Note: Currently, we support exporting to Playground UI with only 1 CSV file, if you have multiple CSV files, please watch out for our next release that will add support to export a NeuralDB directly into Playground."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "138Ch5oDNXC1"
      },
      "outputs": [],
      "source": [
        "from export_utils import neural_db_to_playground\n",
        "\n",
        "neural_db_to_playground(db, './sample_nda/', csv=csv_doc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}